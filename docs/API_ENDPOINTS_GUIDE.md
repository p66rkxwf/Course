# API ç«¯ç‚¹è®¾è®¡æŒ‡å—

## ğŸ“‹ å½“å‰ API ç«¯ç‚¹ï¼ˆæ•°æ®æŸ¥è¯¢ï¼‰

### å·²å®ç°çš„ç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | åŠŸèƒ½ | å‰ç«¯è°ƒç”¨ |
|------|------|------|----------|
| `/api/courses/all` | GET | è·å–æ‰€æœ‰è¯¾ç¨‹ | âœ… æ˜¯ |
| `/api/courses/search` | GET | æœç´¢è¯¾ç¨‹ | âœ… æ˜¯ |
| `/api/courses/recommend` | POST | æ¨èè¯¾ç¨‹ | âœ… æ˜¯ |
| `/api/courses/history` | GET | è·å–å†å²æ•°æ® | âœ… æ˜¯ |
| `/api/courses/stats` | GET | è·å–ç»Ÿè®¡æ•°æ® | âœ… æ˜¯ |
| `/api/departments` | GET | è·å–ç§‘ç³»åˆ—è¡¨ | âœ… æ˜¯ |

**è¿™äº›ç«¯ç‚¹éƒ½æ˜¯å‰ç«¯åº”è¯¥è°ƒç”¨çš„**ï¼Œç”¨äºè·å–å’Œå±•ç¤ºæ•°æ®ã€‚

---

## ğŸ”§ å¯é€‰ï¼šç®¡ç†ç«¯ç‚¹ï¼ˆè§¦å‘åç«¯æ“ä½œï¼‰

å¦‚æœæ‚¨éœ€è¦åœ¨ Web ç•Œé¢ä¸­è§¦å‘ `crawl`ã€`process` ç­‰æ“ä½œï¼Œå¯ä»¥æ·»åŠ ä»¥ä¸‹ç«¯ç‚¹ï¼š

### 1. è§¦å‘çˆ¬è™«ç«¯ç‚¹

```python
# åœ¨ src/api/app.py ä¸­æ·»åŠ 

from fastapi import BackgroundTasks
from pydantic import BaseModel

class TaskResponse(BaseModel):
    status: str
    message: str
    task_id: Optional[str] = None

# ä»»åŠ¡çŠ¶æ€å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis æˆ–æ•°æ®åº“ï¼‰
_task_status = {}

@app.post("/api/admin/crawl", response_model=TaskResponse)
async def trigger_crawl(background_tasks: BackgroundTasks):
    """
    è§¦å‘è¯¾ç¨‹æ•°æ®çˆ¬å–
    
    æ³¨æ„ï¼šæ­¤ç«¯ç‚¹åº”è¯¥æ·»åŠ è®¤è¯ä¿æŠ¤
    """
    try:
        # ä½¿ç”¨åå°ä»»åŠ¡æ‰§è¡Œï¼Œé¿å…é˜»å¡
        def run_crawl():
            from crawler.crawler import main as crawl_main
            crawl_main()
        
        background_tasks.add_task(run_crawl)
        
        return TaskResponse(
            status="started",
            message="çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨åå°æ‰§è¡Œ"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨çˆ¬è™«å¤±è´¥: {str(e)}")

@app.get("/api/admin/crawl/status")
async def get_crawl_status():
    """è·å–çˆ¬è™«ä»»åŠ¡çŠ¶æ€"""
    # å®ç°ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢é€»è¾‘
    return {"status": "running", "progress": 50}
```

### 2. è§¦å‘æ•°æ®å¤„ç†ç«¯ç‚¹

```python
@app.post("/api/admin/process", response_model=TaskResponse)
async def trigger_process(background_tasks: BackgroundTasks):
    """
    è§¦å‘æ•°æ®å¤„ç†
    
    æ³¨æ„ï¼šæ­¤ç«¯ç‚¹åº”è¯¥æ·»åŠ è®¤è¯ä¿æŠ¤
    """
    try:
        def run_process():
            from processor.data_processor import main as process_main
            process_main()
        
        background_tasks.add_task(run_process)
        
        return TaskResponse(
            status="started",
            message="æ•°æ®å¤„ç†ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨åå°æ‰§è¡Œ"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨å¤„ç†å¤±è´¥: {str(e)}")
```

### 3. å‰ç«¯è°ƒç”¨ç¤ºä¾‹

```javascript
// web/assets/js/api.js

export async function triggerCrawl() {
    const response = await fetch(`${API_BASE}/admin/crawl`, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            // æ·»åŠ è®¤è¯å¤´ï¼ˆå¦‚æœå®ç°äº†è®¤è¯ï¼‰
            // 'Authorization': 'Bearer YOUR_API_KEY'
        }
    });
    if (!response.ok) throw new Error('Failed to trigger crawl');
    return await response.json();
}

export async function triggerProcess() {
    const response = await fetch(`${API_BASE}/admin/process`, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json'
        }
    });
    if (!response.ok) throw new Error('Failed to trigger process');
    return await response.json();
}
```

---

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### 1. å®‰å…¨æ€§

**è¿™äº›ç®¡ç†ç«¯ç‚¹å¿…é¡»æ·»åŠ è®¤è¯**ï¼Œå¦åˆ™ä»»ä½•äººéƒ½å¯ä»¥è§¦å‘çˆ¬è™«å’Œæ•°æ®å¤„ç†ï¼š

```python
from fastapi import Depends, HTTPException, Header

API_KEY = "your-secret-api-key"  # åº”è¯¥ä»ç¯å¢ƒå˜é‡è¯»å–

async def verify_api_key(x_api_key: str = Header(...)):
    if x_api_key != API_KEY:
        raise HTTPException(status_code=401, detail="Invalid API Key")
    return x_api_key

@app.post("/api/admin/crawl")
async def trigger_crawl(
    background_tasks: BackgroundTasks,
    api_key: str = Depends(verify_api_key)
):
    # ... å®ç°
```

### 2. å¼‚æ­¥å¤„ç†

é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡åº”è¯¥ä½¿ç”¨åå°ä»»åŠ¡æˆ–ä»»åŠ¡é˜Ÿåˆ—ï¼š

```python
# ä½¿ç”¨ Celeryï¼ˆæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰
from celery import Celery

celery_app = Celery('tasks', broker='redis://localhost:6379')

@celery_app.task
def run_crawl():
    from crawler.crawler import main as crawl_main
    crawl_main()

@app.post("/api/admin/crawl")
async def trigger_crawl():
    task = run_crawl.delay()
    return {"task_id": task.id, "status": "started"}
```

### 3. çŠ¶æ€åé¦ˆ

æä¾›ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢ç«¯ç‚¹ï¼Œè®©å‰ç«¯å¯ä»¥è½®è¯¢ä»»åŠ¡çŠ¶æ€ï¼š

```python
@app.get("/api/admin/tasks/{task_id}")
async def get_task_status(task_id: str):
    # ä» Redis æˆ–æ•°æ®åº“æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
    status = get_task_status_from_storage(task_id)
    return status
```

---

## ğŸ¯ æ¨èæ–¹æ¡ˆ

### æ–¹æ¡ˆ Aï¼šä¸éœ€è¦ Web ç•Œé¢è§¦å‘ï¼ˆæ¨èï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š
- æ•°æ®æ›´æ–°é¢‘ç‡å›ºå®šï¼ˆå¦‚æ¯å­¦æœŸä¸€æ¬¡ï¼‰
- ä½¿ç”¨å®šæ—¶ä»»åŠ¡ï¼ˆCronï¼‰è‡ªåŠ¨æ‰§è¡Œ

**å®æ–½æ–¹å¼**ï¼š
- ä¸éœ€è¦æ·»åŠ ç®¡ç†ç«¯ç‚¹
- ä½¿ç”¨ Cron å®šæ—¶æ‰§è¡Œ `python main.py crawl` å’Œ `python main.py process`
- å‰ç«¯åªè°ƒç”¨æ•°æ®æŸ¥è¯¢ç«¯ç‚¹

### æ–¹æ¡ˆ Bï¼šéœ€è¦ Web ç•Œé¢è§¦å‘

**é€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦æ‰‹åŠ¨è§¦å‘æ•°æ®æ›´æ–°
- éœ€è¦å®æ—¶æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€

**å®æ–½æ–¹å¼**ï¼š
- æ·»åŠ ç®¡ç†ç«¯ç‚¹ï¼ˆå¦‚ä¸Šæ‰€ç¤ºï¼‰
- æ·»åŠ è®¤è¯ä¿æŠ¤
- ä½¿ç”¨åå°ä»»åŠ¡æˆ–ä»»åŠ¡é˜Ÿåˆ—
- å‰ç«¯æ·»åŠ ç®¡ç†ç•Œé¢

---

## ğŸ“ æ€»ç»“

### å‰ç«¯åº”è¯¥è°ƒç”¨çš„ç«¯ç‚¹

âœ… **æ•°æ®æŸ¥è¯¢ç«¯ç‚¹**ï¼ˆå·²å®ç°ï¼‰ï¼š
- `/api/courses/*` - æ‰€æœ‰è¯¾ç¨‹ç›¸å…³æŸ¥è¯¢
- `/api/departments` - ç§‘ç³»æŸ¥è¯¢

### å‰ç«¯ä¸åº”è¯¥ç›´æ¥æ‰§è¡Œçš„æ“ä½œ

âŒ **æœåŠ¡å™¨ç«¯æ“ä½œ**ï¼ˆéœ€è¦é€šè¿‡ API è§¦å‘ï¼‰ï¼š
- çˆ¬è™«æ‰§è¡Œï¼ˆ`crawl`ï¼‰
- æ•°æ®å¤„ç†ï¼ˆ`process`ï¼‰
- å­—å…¸æ„å»ºï¼ˆ`build-dict`ï¼‰

### å®æ–½å»ºè®®

1. **å¦‚æœä¸éœ€è¦ Web ç•Œé¢è§¦å‘**ï¼š
   - ä¿æŒå½“å‰æ¶æ„
   - ä½¿ç”¨å®šæ—¶ä»»åŠ¡æ‰§è¡Œ `crawl` å’Œ `process`
   - å‰ç«¯åªè°ƒç”¨æ•°æ®æŸ¥è¯¢ç«¯ç‚¹

2. **å¦‚æœéœ€è¦ Web ç•Œé¢è§¦å‘**ï¼š
   - æ·»åŠ ç®¡ç†ç«¯ç‚¹ï¼ˆå¦‚ä¸Šæ‰€ç¤ºï¼‰
   - æ·»åŠ è®¤è¯ä¿æŠ¤
   - ä½¿ç”¨åå°ä»»åŠ¡å¤„ç†
   - æä¾›ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢

